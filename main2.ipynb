{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from shutil import copy2\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pocketsphinx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag 1 : Classification Hijaiyah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menampilkan gambar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = r\"C:\\Users\\gagah\\Desktop\\KKN\\data\\Train Images 13440x32x32\\train\"\n",
    "size = 128\n",
    "\n",
    "images_with_filenames = []\n",
    "\n",
    "def extract_id(filename):\n",
    "    match = re.search(r'id_(\\d+)_', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "file_list = [f for f in os.listdir(source_folder) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "file_list.sort(key=extract_id)\n",
    "\n",
    "for filename in file_list:\n",
    "    img_path = os.path.join(source_folder, filename)\n",
    "    img = Image.open(img_path).convert('RGB') \n",
    "    img = img.resize((size, size))\n",
    "    img_array = np.array(img)\n",
    "    images_with_filenames.append((img_array, filename))\n",
    "\n",
    "images = np.array([img[0] for img in images_with_filenames])\n",
    "filenames = [img[1] for img in images_with_filenames]\n",
    "\n",
    "print(f'Total data gambar: {len(images)}')\n",
    "print(f'Shape setiap gambar: {images[0].shape}')\n",
    "\n",
    "\n",
    "# Memunculkan gambar\n",
    "# Memisahkan gambar berdasarkan label\n",
    "label_dict = {}\n",
    "for img_array, filename in images_with_filenames:\n",
    "    match = re.search(r'label_(\\d+)', filename)\n",
    "    if match:\n",
    "        label = int(match.group(1))\n",
    "        if label not in label_dict:\n",
    "            label_dict[label] = []\n",
    "        label_dict[label].append((img_array, filename))\n",
    "\n",
    "# Menampilkan gambar per label\n",
    "for label, images in label_dict.items():\n",
    "    fig, axes = plt.subplots(1, 8, figsize=(10, 2))\n",
    "    axes = axes.ravel()\n",
    "    for i in range(min(8, len(images))):\n",
    "        axes[i].imshow(images[i][0])\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(images[i][1], fontsize=8)\n",
    "    plt.suptitle(f'Label {label}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memisahkan dataset sesuai label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memisahkan data di folder utama menjadi sub folder\n",
    "source_folder = r\"C:\\Users\\gagah\\Desktop\\KKN\\data\\Train Images 13440x32x32\\train\"\n",
    "destination_folder = r\"C:\\Users\\gagah\\Desktop\\KKN\\data\\Data_Train_Images\"\n",
    "\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "file_list = [f for f in os.listdir(source_folder) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "\n",
    "for filename in file_list:\n",
    "    match = re.search(r'id_(\\d+)_label_(\\d+)', filename)\n",
    "    if match:\n",
    "        img_id = match.group(1)\n",
    "        label = match.group(2)\n",
    "        \n",
    "        # Membuat path folder label\n",
    "        label_folder = os.path.join(destination_folder, f'lab_{label}')\n",
    "        if not os.path.exists(label_folder):\n",
    "            os.makedirs(label_folder)\n",
    "        \n",
    "        # Menyalin file ke folder tujuan dengan nama file baru\n",
    "        src_path = os.path.join(source_folder, filename)\n",
    "        dest_path = os.path.join(label_folder, f'id_{img_id}.png')\n",
    "        copy2(src_path, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membuat model DeepLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = \"data\\Data_Train_Images\"\n",
    "\n",
    "model_dir = 'model'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rotation_range=20,      # Meningkatkan rotation range\n",
    "    shear_range=0.4,\n",
    "    zoom_range=0.2,         # Menambahkan zoom range\n",
    "    width_shift_range=0.2,  # Menambahkan width shift\n",
    "    height_shift_range=0.2, # Menambahkan height shift\n",
    "    fill_mode='nearest',    # Mode pengisian\n",
    "    validation_split=0.2    # Split validation\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    directory=TRAINING_DIR,\n",
    "    target_size=(size, size),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training' \n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    directory=TRAINING_DIR,\n",
    "    target_size=(size, size),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(size, size, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=l2(0.03)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.03)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(0.03)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(label_dict.keys()), activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.path.join(model_dir, 'model_{epoch:02d}_{val_accuracy:.2f}.h5'),  # Nama file dengan epoch dan akurasi validasi\n",
    "    monitor='val_accuracy',    # Metrik yang dipantau\n",
    "    save_best_only=True,       # Hanya menyimpan model terbaik\n",
    "    mode='max',                # Mode 'max' karena kita ingin akurasi maksimum\n",
    "    verbose=1                  # Menampilkan log setiap kali model disimpan\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'lab_1': 'alif',\n",
    "    'lab_2': 'ba',\n",
    "    'lab_3': 'ta',\n",
    "    'lab_4': 'tsa',\n",
    "    'lab_5': 'jim',\n",
    "    'lab_6': 'ha',\n",
    "    'lab_7': 'kho',\n",
    "    'lab_8': 'dal',\n",
    "    'lab_9': 'dzal',\n",
    "    'lab_10': 'ra',\n",
    "    'lab_11': 'zai',\n",
    "    'lab_12': 'sin',\n",
    "    'lab_13': 'syin',\n",
    "    'lab_14': 'shad',\n",
    "    'lab_15': 'dhad',\n",
    "    'lab_16': 'tha',\n",
    "    'lab_17': 'dha',\n",
    "    'lab_18': 'ain',\n",
    "    'lab_19': 'ghain',\n",
    "    'lab_20': 'fa',\n",
    "    'lab_21': 'qaf',\n",
    "    'lab_22': 'kaf',\n",
    "    'lab_23': 'lam',\n",
    "    'lab_24': 'mim',\n",
    "    'lab_25': 'nun',\n",
    "    'lab_26': 'ha',\n",
    "    'lab_27': 'waw',\n",
    "    'lab_28': 'ya'\n",
    "}\n",
    "\n",
    "def load_and_preprocess_image(img_path, target_size=(128, 128)):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img)\n",
    "    img_array = img_array / 255.0  # Normalize pixel values\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "def load_images_from_folders(base_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = os.listdir(base_folder)\n",
    "    \n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        class_folder = os.path.join(base_folder, class_name)\n",
    "        if os.path.isdir(class_folder):\n",
    "            for filename in os.listdir(class_folder):\n",
    "                if filename.endswith(\".png\"):\n",
    "                    img_path = os.path.join(class_folder, filename)\n",
    "                    img_array = load_and_preprocess_image(img_path)\n",
    "                    images.append(img_array)\n",
    "                    labels.append(class_index)\n",
    "    \n",
    "    images = np.vstack(images)  # Combine all images into one large array\n",
    "    labels = np.array(labels)\n",
    "    return images, labels, class_names\n",
    "\n",
    "# Load the model\n",
    "model = load_model('model/model_38_0.86.h5')\n",
    "\n",
    "# Path to the dataset folder\n",
    "base_folder = 'data/Data_Train_Images'\n",
    "\n",
    "# Load and preprocess images\n",
    "images, true_labels, class_names = load_images_from_folders(base_folder)\n",
    "\n",
    "# Predict the class\n",
    "predictions = model.predict(images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_classes)\n",
    "\n",
    "# Display confusion matrix with new labels\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report with new labels\n",
    "print(classification_report(true_labels, predicted_classes, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "lab_mapping = {}\n",
    "with open('label_mapping.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':')\n",
    "            lab_mapping[int(key.strip())] = value.strip().strip(\"'\").replace(\"'\", \"\").replace(\",\", \"\")\n",
    "\n",
    "# Ambil sampel acak dari folder\n",
    "sample_size = 20  # Jumlah sampel yang ingin diambil\n",
    "sample_files = random.sample(os.listdir(source_folder), sample_size)\n",
    "\n",
    "# Muat gambar dan proses untuk prediksi\n",
    "images = []\n",
    "for file in sample_files:\n",
    "    img_path = os.path.join(source_folder, file)\n",
    "    img = load_img(img_path, target_size=(128, 128))  # Sesuaikan ukuran target dengan model Anda\n",
    "    img_array = img_to_array(img) / 255.0  # Rescale gambar\n",
    "    images.append(img_array)\n",
    "\n",
    "# Convert list images ke numpy array\n",
    "images = np.array(images)\n",
    "\n",
    "# Prediksi menggunakan model\n",
    "predictions = model.predict(images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Menampilkan gambar dengan label asli dan prediksi\n",
    "plt.figure(figsize=[14, 14])\n",
    "for i in range(sample_size):\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.axis('off')\n",
    "    plt.title(\"File: {}\\nPrediction: {} \\n({:.1f}%)\".format(\n",
    "        sample_files[i],\n",
    "        lab_mapping[predicted_classes[i]],\n",
    "        np.max(predictions[i]) * 100\n",
    "    ))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ubah model menjadi TfLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model Keras (.h5)\n",
    "model = tf.keras.models.load_model('model/model_38_0.86.h5')\n",
    "\n",
    "# Konversi model ke format TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Simpan model TensorFlow Lite (.tflite)\n",
    "with open('model_86.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model berhasil dikonversi ke .tflite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
